## Normal data
The average expected reward of each trajectory from the demonstrator is $281.18\pm22.93$. This dataset has a continuous state space and discrete action space. 
## Policy Anomalies
For the LunarLander dataset, the policy anomalies are generated by sub-optimal agents where the anomalous level is quantified by the average expected rewards of each trajectory, where lower rewards indicate a more anomalous trajectory. The sub-optimal agents used in LunarLander have an anomalous level of $56.94\pm90.38$. 
## Pertubed Anomalies
We add Gaussian noise to the state measurements of normal trajectories to generate perturbed anomalies.
We set the Gaussian noise to $N(0, 0.1)$ for the LunarLander dataset.
